{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "302168a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gym\n",
    "import gym_carla\n",
    "import carla\n",
    "import scipy.signal\n",
    "import time\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b354c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/c557/lib/python3.6/site-packages/tensorflow/python/client/session.py:1766: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d37d5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53ffbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195cf571",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions_steer = 3\n",
    "num_actions_acc = 2\n",
    "observation_dimensions = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb52db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, *observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.steer_action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.acc_action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer_acc = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer_steer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, acc_action, steer_action, reward, value, logprobability_acc, logprobability_steer):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.steer_action_buffer[self.pointer] = steer_action\n",
    "        self.acc_action_buffer[self.pointer] = acc_action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer_acc[self.pointer] = logprobability_acc\n",
    "        self.logprobability_buffer_steer[self.pointer] = logprobability_steer\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.acc_action_buffer,\n",
    "            self.steer_action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer_acc,\n",
    "            self.logprobability_buffer_steer\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3,3), kernel_initializer='he_normal', \n",
    "                            padding='same', activation=\"relu\")(x)\n",
    "    x = keras.layers.AveragePooling2D( (2,2) )(x)\n",
    "    x = keras.layers.Conv2D(filters=8, kernel_size=(3,3), kernel_initializer='he_normal', \n",
    "                            padding='same', activation=\"relu\")(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    for size in sizes[:-1]:\n",
    "        x = keras.layers.Dense(units=size, activation=activation)(x)\n",
    "    return keras.layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities_steer(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions_steer) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "def logprobabilities_acc(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions_acc) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(actor, observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer_acc, action_buffer_steer, logprobability_buffer_acc, logprobability_buffer_steer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities_acc(actor_acc(observation_buffer), action_buffer_acc)\n",
    "            - logprobability_buffer_acc\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor_acc.trainable_variables)\n",
    "    policy_optimizer_acc.apply_gradients(zip(policy_grads, actor_acc.trainable_variables))\n",
    "    kl1 = tf.reduce_mean(\n",
    "        logprobability_buffer_acc\n",
    "        - logprobabilities_acc(actor_acc(observation_buffer), action_buffer_acc)\n",
    "    )\n",
    "    kl1 = tf.reduce_sum(kl1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities_steer(actor_steer(observation_buffer), action_buffer_steer)\n",
    "            - logprobability_buffer_steer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor_steer.trainable_variables)\n",
    "    policy_optimizer_steer.apply_gradients(zip(policy_grads, actor_steer.trainable_variables))\n",
    "\n",
    "    kl2 = tf.reduce_mean(\n",
    "        logprobability_buffer_steer\n",
    "        - logprobabilities_steer(actor_steer(observation_buffer), action_buffer_steer)\n",
    "    )\n",
    "    kl2 = tf.reduce_sum(kl2)\n",
    "    \n",
    "    return kl1+kl2\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab542b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 2000\n",
    "epochs = 100\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 20\n",
    "train_value_iterations = 20\n",
    "lam = 0.97\n",
    "hidden_sizes = (128, 64)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acea629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'number_of_vehicles': 40,\n",
    "    'number_of_walkers': 50,\n",
    "    'display_size': 250,  # screen size of bird-eye render\n",
    "    'display_main': False,\n",
    "    'weather': \"WetSunset\",\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': False,  # whether to use discrete control space\n",
    "    'discrete_acc': [1.0, 0.0, 1.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-1, 0, 1],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.2, 0.2],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.tesla.model3',  # filter for defining ego vehicle\n",
    "    'address': 'localhost',\n",
    "    'port': 2000, # connection port\n",
    "    'town': 'Town02', # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 5000,  # maximum timesteps per episode\n",
    "    'max_waypt': 12,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 5.0,  # threshold for out of lane\n",
    "    'desired_speed': 8,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 200,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9cc9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "observation_input = keras.layers.Input( shape=observation_dimensions )\n",
    "\n",
    "logits_acc = mlp(observation_input, list(hidden_sizes)+[num_actions_acc], tf.tanh, None)\n",
    "actor_acc = keras.Model(inputs=observation_input, outputs=logits_acc)\n",
    "\n",
    "logits_steer = mlp(observation_input, list(hidden_sizes)+[num_actions_steer], tf.tanh, None)\n",
    "actor_steer = keras.Model(inputs=observation_input, outputs=logits_steer)\n",
    "\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes)+[1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer_acc = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "policy_optimizer_steer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc249f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_13 (Averag (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 64, 64, 8)         2312      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 4,206,091\n",
      "Trainable params: 4,206,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_steer.summary()\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "406e0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img):\n",
    "    a = img[76:204,76:204,:]/255\n",
    "    return a.reshape( (1,128,128,3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f943e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6adda112",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_steer.load_weights(\"models/ppo_no_vae_actor_steer_1636864694.h5\")\n",
    "actor_acc.load_weights(\"models/ppo_no_vae_actor_acc_1636864694.h5\")\n",
    "critic.load_weights(\"models/ppo_no_vae_critic_1636864694.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb33bdbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to Carla server...\n",
      "Carla server connected!\n",
      " Epoch: 53. Mean Return: 1334.911031992476. Mean Length: 666.6666666666666\n",
      " Epoch: 54. Mean Return: 1614.0934876511221. Mean Length: 666.6666666666666\n",
      " Epoch: 55. Mean Return: 1393.7389885892635. Mean Length: 666.6666666666666\n",
      " Epoch: 56. Mean Return: 2401.0299657273176. Mean Length: 1000.0\n",
      " Epoch: 57. Mean Return: 1126.4823493574763. Mean Length: 500.0\n",
      " Epoch: 58. Mean Return: 1659.5144688754253. Mean Length: 666.6666666666666\n",
      " Epoch: 59. Mean Return: 1196.7707605010664. Mean Length: 666.6666666666666\n",
      " Epoch: 60. Mean Return: 3444.9761675593536. Mean Length: 2000.0\n",
      " Epoch: 61. Mean Return: 2283.093096170719. Mean Length: 1000.0\n",
      " Epoch: 62. Mean Return: 1888.7658626367784. Mean Length: 1000.0\n",
      " Epoch: 63. Mean Return: 2622.774484542753. Mean Length: 1000.0\n",
      " Epoch: 64. Mean Return: 1462.2795725754283. Mean Length: 666.6666666666666\n",
      " Epoch: 65. Mean Return: 2525.6960619597603. Mean Length: 1000.0\n",
      " Epoch: 66. Mean Return: 886.8381362271593. Mean Length: 500.0\n",
      " Epoch: 67. Mean Return: 2058.8713373109604. Mean Length: 1000.0\n",
      " Epoch: 68. Mean Return: 1286.0096372336093. Mean Length: 666.6666666666666\n",
      " Epoch: 69. Mean Return: 4303.003968211807. Mean Length: 2000.0\n",
      " Epoch: 70. Mean Return: 1301.379616657715. Mean Length: 666.6666666666666\n",
      " Epoch: 71. Mean Return: 4135.738205285232. Mean Length: 2000.0\n",
      " Epoch: 72. Mean Return: 1101.6930721882968. Mean Length: 666.6666666666666\n",
      " Epoch: 73. Mean Return: 1448.5911836102769. Mean Length: 666.6666666666666\n",
      " Epoch: 74. Mean Return: 87.89881805514268. Mean Length: 250.0\n",
      " Epoch: 75. Mean Return: 1459.125158627912. Mean Length: 666.6666666666666\n",
      " Epoch: 76. Mean Return: 1342.8131086968413. Mean Length: 666.6666666666666\n",
      " Epoch: 77. Mean Return: 858.6225585339856. Mean Length: 400.0\n",
      " Epoch: 78. Mean Return: 442.63082091215676. Mean Length: 400.0\n",
      " Epoch: 79. Mean Return: 5045.992529002951. Mean Length: 2000.0\n",
      " Epoch: 80. Mean Return: 2069.9777925765106. Mean Length: 1000.0\n",
      " Epoch: 81. Mean Return: 325.03178186739405. Mean Length: 222.22222222222223\n",
      " Epoch: 82. Mean Return: 549.3130518813364. Mean Length: 333.3333333333333\n",
      " Epoch: 83. Mean Return: 1756.6353934929616. Mean Length: 1000.0\n",
      " Epoch: 84. Mean Return: 608.5206738894866. Mean Length: 400.0\n",
      " Epoch: 85. Mean Return: 1422.859005479424. Mean Length: 666.6666666666666\n",
      " Epoch: 86. Mean Return: 1641.4897440132809. Mean Length: 1000.0\n",
      " Epoch: 87. Mean Return: 1671.3041078164836. Mean Length: 666.6666666666666\n",
      " Epoch: 88. Mean Return: 3587.4411595508836. Mean Length: 2000.0\n",
      " Epoch: 89. Mean Return: 2177.499298557699. Mean Length: 1000.0\n",
      " Epoch: 90. Mean Return: 4962.84165070603. Mean Length: 2000.0\n",
      " Epoch: 91. Mean Return: 481.48713234585534. Mean Length: 333.3333333333333\n",
      " Epoch: 92. Mean Return: 1530.598255711986. Mean Length: 1000.0\n",
      " Epoch: 93. Mean Return: 1367.256962479454. Mean Length: 666.6666666666666\n",
      " Epoch: 94. Mean Return: 2639.750400166745. Mean Length: 1000.0\n",
      " Epoch: 95. Mean Return: 2484.1983897814116. Mean Length: 1000.0\n",
      " Epoch: 96. Mean Return: 852.8791489056814. Mean Length: 500.0\n",
      " Epoch: 97. Mean Return: 1692.6987650368037. Mean Length: 666.6666666666666\n",
      " Epoch: 98. Mean Return: 1138.493062124834. Mean Length: 500.0\n",
      " Epoch: 99. Mean Return: 411.3617995676674. Mean Length: 285.7142857142857\n",
      " Epoch: 100. Mean Return: 549.1689967784021. Mean Length: 285.7142857142857\n"
     ]
    }
   ],
   "source": [
    "# Initialize the observation, episode return and episode length\n",
    "env = gym.make('carla-v0', params=params)\n",
    "observation =  env.reset() # select_env()\n",
    "episode_return, episode_length = 0, 0\n",
    "for _ in range(20): observation, _, _, _ = env.step([1,0])\n",
    "observation = transform( observation['birdeye'] )\n",
    "\n",
    "while epoch != epochs:\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "        \n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch): #while True\n",
    "        \n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        logits_acc, action_acc = sample_action(actor_acc, observation)\n",
    "        logits_steer, action_steer = sample_action(actor_steer, observation)\n",
    "        \n",
    "#         move_steer, move_acc = None, None\n",
    "#         if np.random.random() > 1/epoch:\n",
    "#             move_acc = np.argmax(classifier_acc.predict( observation )[0])\n",
    "#             move_steer = np.argmax(classifier_steer.predict( observation )[0])-1\n",
    "#         else: \n",
    "        move_acc = action_acc[0].numpy()\n",
    "        move_steer = action_steer[0].numpy()-1\n",
    "\n",
    "        observation_new, reward, done, _ = env.step([1.25*move_acc if move_acc == 1 else -1, move_steer])\n",
    "        observation_new = transform(observation_new['birdeye'])\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t_acc = logprobabilities_acc(logits_acc, action_acc)\n",
    "        logprobability_t_steer = logprobabilities_steer(logits_steer, action_steer)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action_acc, action_steer, reward, value_t, logprobability_t_acc, logprobability_t_steer)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation)\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation =  env.reset()\n",
    "            episode_return, episode_length = 0, 0\n",
    "            for _ in range(20): observation, _, _, _ = env.step([1,0])\n",
    "            observation = transform(observation['birdeye'])\n",
    "            \n",
    "            \n",
    "    # Print mean return and length for each epoch\n",
    "    with open(\"logs_no_vae_ppo.txt\", \"a\") as log:\n",
    "        print(\n",
    "            f\"{epoch + 1},{sum_return/num_episodes},{sum_length/num_episodes},{num_episodes}\",\n",
    "            file = log\n",
    "        )\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return/num_episodes}. Mean Length: {sum_length/num_episodes}\"\n",
    "    )\n",
    "    \n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer_acc,\n",
    "        action_buffer_steer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer_acc,\n",
    "        logprobability_buffer_steer\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(2):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer_acc, action_buffer_steer, logprobability_buffer_acc, logprobability_buffer_steer, advantage_buffer\n",
    "        )\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(2):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "    \n",
    "    tm = int(time.time())\n",
    "    actor_acc.save_weights(f\"models/ppo_no_vae_actor_acc_{epoch}_{tm}.h5\")\n",
    "    actor_steer.save_weights(f\"models/ppo_no_vae_actor_steer_{epoch}_{tm}.h5\")\n",
    "    critic.save_weights(f\"models/ppo_no_vae_critic_{epoch}_{tm}.h5\")\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1496e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fd208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
